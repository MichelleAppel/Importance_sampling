{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "n-centered moment MNIST",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichelleAppel/Importance_sampling/blob/master/toy_examples/n_centered_moment_MNIST_AE_bastian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFffV8slk-gP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from itertools import chain\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "\n",
        "from scipy import signal\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oValuDnTk-gO"
      },
      "source": [
        "# Toy example MNIST with PCA embeddings (one-sided)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVvwqfTjHbxK"
      },
      "source": [
        "def MNIST_data(distribution=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], one_hot_labels=False):\n",
        "    # distribution: distribution for each label\n",
        "    # returns (data, labels) for MNIST with all the classes; zeroes and ones have the given distribution\n",
        "\n",
        "    MNIST = torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "    \n",
        "    distribution /= np.array(distribution).sum()\n",
        "    \n",
        "    # bool mask for each label\n",
        "    idxm = [MNIST.targets==label for label in range(len(distribution))]\n",
        "    min_idx = torch.Tensor([sum(i) for i in idxm]).min().int().item()\n",
        "    tot_idx = torch.Tensor([sum(i) for i in idxm]).sum().int().item()\n",
        "\n",
        "    # list of indices for each label\n",
        "    idx = [np.where(idxm[label])[0] for label in range(len(distribution))]\n",
        "    \n",
        "    len_idx = torch.Tensor([len(i) for i in idx])\n",
        "    wanted_len = torch.Tensor([d*tot_idx for d in distribution])\n",
        "    \n",
        "    min_class = ((len_idx/wanted_len).argmin())\n",
        "    min = ((len_idx/wanted_len).min().item())\n",
        "\n",
        "    if min < 1:\n",
        "      distribution *= min\n",
        "\n",
        "    valid_idx = []\n",
        "    class_len = [math.floor(d*tot_idx) for d in distribution]\n",
        "    for label, length in enumerate(class_len):\n",
        "      valid_idx += idx[label][:length].tolist()\n",
        "\n",
        "    valid_idx = np.array(valid_idx)\n",
        "    np.random.shuffle(valid_idx)\n",
        "\n",
        "    # assign the new data and labels to the dataset\n",
        "    if one_hot_labels:\n",
        "      MNIST.targets = torch.nn.functional.one_hot(MNIST.targets[valid_idx])\n",
        "    else:\n",
        "      MNIST.targets = MNIST.targets[valid_idx]\n",
        "    MNIST.data = MNIST.data[valid_idx]\n",
        "\n",
        "    return MNIST "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2kQeQqIk-gZ"
      },
      "source": [
        "class MNISTDataset(Dataset):\n",
        "    '''The dataset for the MNIST binary data\n",
        "    '''\n",
        "    def __init__(self, distribution=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], one_hot_labels=False):\n",
        "\n",
        "        self.distribution = distribution        \n",
        "        self.dataset = MNIST_data(distribution=self.distribution, one_hot_labels=one_hot_labels)\n",
        "        self.example_imgs = self.example()\n",
        "        \n",
        "        # to take out in real applications\n",
        "        self.unique_labels = torch.unique(self.dataset.targets)[:len(distribution)]\n",
        "\n",
        "    def example(self):\n",
        "        '''\n",
        "        Returns an example from each digit in the domain\n",
        "        \n",
        "        '''\n",
        "        labels = self.dataset.targets\n",
        "        data = self.dataset.data\n",
        "        '''img0 = data[labels==0][0].unsqueeze(0)\n",
        "        img1 = data[labels==1][0].unsqueeze(0)\n",
        "        img2 = data[labels==2][0].unsqueeze(0)\n",
        "        img3 = data[labels==3][0].unsqueeze(0)\n",
        "        img4 = data[labels==4][0].unsqueeze(0)\n",
        "        img5 = data[labels==5][0].unsqueeze(0)\n",
        "        img6 = data[labels==6][0].unsqueeze(0)\n",
        "        img7 = data[labels==7][0].unsqueeze(0)\n",
        "        img8 = data[labels==8][0].unsqueeze(0)\n",
        "        img9 = data[labels==9][0].unsqueeze(0)\n",
        "        ex = torch.cat((img0, img1, img2, img3, img4, img5, img6, img7, img8, img9), 0)'''\n",
        "        img = []\n",
        "        for label in torch.unique(labels)[:len(self.distribution)]:\n",
        "          img = img + [data[labels==label][0].unsqueeze(0)]\n",
        "        ex = torch.cat(img, 0)\n",
        "              \n",
        "        return ex\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, idx):      \n",
        "        return self.dataset[idx]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV_U-4hNHbx1"
      },
      "source": [
        "In this example we have 2 domains with 10 classes. Only the first two classes have a different probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbbBXC3zk-gb"
      },
      "source": [
        "batch_size = 256\n",
        "\n",
        "# Settings for domain A\n",
        "dataset_A = MNISTDataset(distribution=[0.05, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
        "# dataset_A = MNISTDataset(distribution=[0.2, 0.8])\n",
        "dataloader_A = DataLoader(dataset_A, batch_size, shuffle=True)\n",
        "\n",
        "# Settings for domain B\n",
        "dataset_B = MNISTDataset(distribution=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
        "# dataset_B = MNISTDataset(distribution=[0.5, 0.5])\n",
        "dataloader_B = DataLoader(dataset_B, batch_size, shuffle=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIgeSM6Kk-gi"
      },
      "source": [
        "class WeightNet(nn.Module):\n",
        "    '''A simple network that predicts the importances of the samples'''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(WeightNet, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(320, 40)\n",
        "        self.fc2 = nn.Linear(40, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h1 = torch.sigmoid(F.max_pool2d(self.conv1(x), 2))\n",
        "        h2 = torch.sigmoid(F.max_pool2d(self.conv2(h1), 2))\n",
        "        h3 = h2.view(-1, 320)\n",
        "        h4 = torch.sigmoid(self.fc1(h3))\n",
        "        out = self.fc2(h4)\n",
        "        return self.softmax(out), out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1TMUerSEych"
      },
      "source": [
        "class Encoder_MNIST(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(Encoder_MNIST, self).__init__()\r\n",
        "\r\n",
        "    self.conv1 = nn.Conv2d(1, 8, 3, stride=2, padding=1)\r\n",
        "    self.conv2 = nn.Conv2d(8, 16, 3, stride=2, padding=1)\r\n",
        "    self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\r\n",
        "    self.fc = nn.Linear(32*4*4, 16)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = nn.ReLU()(self.conv1(x))\r\n",
        "    x = nn.ReLU()(self.conv2(x))\r\n",
        "    x = nn.ReLU()(self.conv3(x))\r\n",
        "    x = nn.Flatten()(x)\r\n",
        "    x = self.fc(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "class Decoder_MNIST(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(Decoder_MNIST, self).__init__()\r\n",
        "\r\n",
        "    self.fc = nn.Linear(16, 32*4*4)\r\n",
        "    self.conv1 = nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1)\r\n",
        "    self.conv2 = nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1)\r\n",
        "    self.conv3 = nn.ConvTranspose2d(8, 1, 3, stride=2, padding=0)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = nn.ReLU()(self.fc(x))\r\n",
        "    x = x.reshape(-1, 32, 4, 4)\r\n",
        "    x = nn.ReLU()(self.conv1(x))\r\n",
        "    x = nn.ReLU()(self.conv2(x))\r\n",
        "    x = self.conv3(x, output_size=(28, 28))\r\n",
        "    return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApJOqpacoJ4C"
      },
      "source": [
        "def n_centered_moment(x, w, n):\n",
        "  if n > 1:\n",
        "    c = n_centered_moment(x, w, 1)\n",
        "  else:\n",
        "    c = 0\n",
        "  return (((x - c)**n)*w).sum(0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fbAA3yD43jw"
      },
      "source": [
        "def raw_to_onehot(labels, n_classes):\n",
        "  onehot = torch.zeros(len(labels), n_classes)\n",
        "  for c in labels.unique():\n",
        "    onehot[labels==c, c] = 1\n",
        "  return onehot"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV18Gu-Xk-gl"
      },
      "source": [
        "# Initialize the networks\n",
        "weight_network = WeightNet().cuda()\n",
        "encoder = Encoder_MNIST().cuda()\n",
        "decoder = Decoder_MNIST().cuda()\n",
        "\n",
        "# Initialize the optimizers\n",
        "lr = 0.01\n",
        "optimizer_w = optim.Adam(weight_network.parameters(), lr=lr)\n",
        "optimizer_ae = optim.Adam(chain(encoder.parameters(), decoder.parameters()), lr=lr)\n",
        "\n",
        "criterion_w = nn.MSELoss()\n",
        "criterion_ae = nn.BCELoss()\n",
        "\n",
        "# For storing results\n",
        "losses_w = []\n",
        "losses_ae = []\n",
        "\n",
        "means_A = []\n",
        "means_B = []\n",
        "\n",
        "vars_A = []\n",
        "vars_B = []\n",
        "\n",
        "moments_A = []\n",
        "moments_B = []\n",
        "\n",
        "example_importances_A = []\n",
        "\n",
        "n = 2 # n-centered moment\n",
        "n_classes = len(dataset_A.distribution)\n",
        "\n",
        "for epoch in range(1):\n",
        "    for i, (real_A, real_B) in enumerate(zip(dataloader_A, dataloader_B)):\n",
        "        \n",
        "        img_A = real_A[0].cuda()\n",
        "        img_B = real_B[0].cuda()\n",
        "\n",
        "        label_A = raw_to_onehot(real_A[1], n_classes).cuda()\n",
        "        label_B = raw_to_onehot(real_B[1], n_classes).cuda()\n",
        "\n",
        "        # The embeddings\n",
        "        e_A = encoder(img_A)\n",
        "        e_B = encoder(img_B)\n",
        "\n",
        "        reconstructed_A = decoder(e_A)\n",
        "        reconstructed_B = decoder(e_B)\n",
        "\n",
        "        # The weighting process\n",
        "        w_A = weight_network(img_A)[0]\n",
        "        w_B = 1/len(img_B)\n",
        "\n",
        "        # The loss function --------------------------------------------------------------------------------\n",
        "        n_centered_moment_A = n_centered_moment(e_A.detach(), w_A, n)\n",
        "        n_centered_moment_B = n_centered_moment(e_B.detach(), w_B, n)\n",
        "        loss_w = criterion_w(n_centered_moment_A, n_centered_moment_B)\n",
        "\n",
        "        loss_ae_A = criterion_ae(reconstructed_A, img_A)\n",
        "        loss_ae_B = criterion_ae(reconstructed_B, img_B)\n",
        "        loss_ae = loss_ae_A + loss_ae_B\n",
        "        # ---------------------------------------------------------------------------------------------------\n",
        "\n",
        "        # Backward\n",
        "        optimizer_w.zero_grad()\n",
        "        loss_w.backward()\n",
        "        optimizer_w.step() \n",
        "\n",
        "        optimizer_ae.zero_grad()\n",
        "        loss_ae.backward()\n",
        "        optimizer_ae.step()     \n",
        "\n",
        "        # Store values --------------------------------------------------------------------------------------\n",
        "        moments_A += [n_centered_moment_A.cpu().detach().numpy()]\n",
        "        moments_B += [n_centered_moment_B.cpu().detach().numpy()]\n",
        "\n",
        "        means_A += [n_centered_moment(label_A, w_A, 1).detach().cpu().numpy()]\n",
        "        means_B += [n_centered_moment(label_B, w_B, 1).detach().cpu().numpy()]\n",
        "\n",
        "        vars_A += [n_centered_moment(label_A, w_A, 2).detach().cpu().numpy()]\n",
        "        vars_B += [n_centered_moment(label_B, w_B, 2).detach().cpu().numpy()]    \n",
        "\n",
        "        losses_w += [loss_w.item()]\n",
        "        losses_ae += [loss_ae.item()]\n",
        "\n",
        "        w_a = weight_network(dataset_A.example_imgs.cuda().unsqueeze(1).float())\n",
        "        example_importances_A += [[importance.item() for importance in w_a[0]]] # Store examples in a list\n",
        "\n",
        "        # ---------------------------------------------------------------------------------------------------\n",
        "\n",
        "        # Print statistics\n",
        "        if i % 50 == 0:\n",
        "            print('epoch', epoch, 'step', i, 'loss_w: ', loss_w.item(), 'loss_ae', loss_ae.item())\n",
        "            \n",
        "        if i % 10000 == 0 and i != 0:\n",
        "            break"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIcZei-plaxu"
      },
      "source": [
        "moments_A = torch.Tensor(moments_A)\r\n",
        "moments_B = torch.Tensor(moments_B)\r\n",
        "means_A = torch.Tensor(means_A)\r\n",
        "means_B = torch.Tensor(means_B)\r\n",
        "vars_A = torch.Tensor(vars_A)\r\n",
        "vars_B = torch.Tensor(vars_B)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzDBcvmDHby4"
      },
      "source": [
        "## Results\n",
        "In the plot below we see that the loss of W is going down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx6S16dbk-go"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Losses over iterations')\n",
        "plt.xlabel('Training iterations')\n",
        "plt.ylabel('Loss')\n",
        "# plt.yscale('symlog')\n",
        "smoothed_losses_w = signal.savgol_filter(losses_w,101,3)\n",
        "plt.plot(smoothed_losses_w)\n",
        "plt.legend(['W'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GKJ5TTsbx1U"
      },
      "source": [
        "plt.figure(figsize=(10,6))\r\n",
        "plt.title('Losses over iterations')\r\n",
        "plt.xlabel('Training iterations')\r\n",
        "plt.ylabel('Loss')\r\n",
        "# plt.yscale('symlog')\r\n",
        "smoothed_losses_ae = signal.savgol_filter(losses_ae,101,3)\r\n",
        "plt.plot(smoothed_losses_ae)\r\n",
        "plt.legend(['AE'])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM4uVAKQHbzK"
      },
      "source": [
        "The plot below shows that the classes 0 and 1 in domain A are weighted to match the uniform distribution in domain B."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llvZ7ah0k-gq"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Moments A - measure of domain A after weighting (smoothed)')\n",
        "plt.xlabel('Training iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(ymax = moments_A.max(), ymin = moments_A.min())\n",
        "smoothed_Lmin = signal.savgol_filter(moments_A,101,3,axis=0)\n",
        "plt.plot(smoothed_Lmin)\n",
        "# plt.legend(np.arange(len(moments_A[0])))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Moments B - Measure of domain B (smoothed)')\n",
        "plt.xlabel('Training iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(ymax = moments_A.max(), ymin = moments_A.min())\n",
        "smoothed_Lplus = signal.savgol_filter(moments_B,101,3,axis=0)\n",
        "plt.plot(smoothed_Lplus)\n",
        "# plt.legend(np.arange(len(moments_A[0])))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVZv4G8pPoCi"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Mean of domain A after weighting (smoothed)')\n",
        "plt.xlabel('Training iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(ymax = means_A.max(), ymin = means_A.min())\n",
        "smoothed_means_A = signal.savgol_filter(means_A,101,3,axis=0)\n",
        "plt.plot(smoothed_means_A)\n",
        "# plt.legend(np.arange(n_classes))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Mean of domain B (smoothed)')\n",
        "plt.xlabel('Training iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(ymax = means_A.max(), ymin = means_A.min())\n",
        "smoothed_means_B = signal.savgol_filter(means_B,101,3,axis=0)\n",
        "plt.plot(smoothed_means_B)\n",
        "# plt.legend(np.arange(n_classes))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvyg5fJRQBDo"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Var of domain A after weighting (smoothed)')\n",
        "plt.xlabel('Training iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(ymax = vars_A.max(), ymin = vars_A.min())\n",
        "smoothed_vars_A = signal.savgol_filter(vars_A,101,3,axis=0)\n",
        "plt.plot(smoothed_vars_A)\n",
        "plt.legend(np.arange(n_classes))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Var of domain B (smoothed)')\n",
        "plt.xlabel('Training iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(ymax = vars_A.max(), ymin = vars_A.min())\n",
        "smoothed_vars_B = signal.savgol_filter(vars_B,101,3,axis=0)\n",
        "plt.plot(smoothed_vars_B)\n",
        "plt.legend(np.arange(n_classes))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFCkwwHvR48_"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Mean and var of domain A after weighting (smoothed)')\n",
        "plt.xlabel('Training iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(ymax = (smoothed_means_A + smoothed_vars_A).max(), ymin = (smoothed_means_A - smoothed_vars_A).min())\n",
        "plt.plot(smoothed_means_A)\n",
        "for i in range(smoothed_means_A.shape[1]):\n",
        "  plt.fill_between(np.arange(len(smoothed_means_A)), (smoothed_means_A - smoothed_vars_A)[:, i], (smoothed_means_A + smoothed_vars_A)[:, i], alpha=0.1)\n",
        "plt.legend(np.arange(n_classes))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Mean and var of domain A after weighting (smoothed)')\n",
        "plt.xlabel('Training iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(ymax = (smoothed_means_A + smoothed_vars_A).max(), ymin = (smoothed_means_A - smoothed_vars_A).min())\n",
        "plt.plot(smoothed_means_B)\n",
        "for i in range(smoothed_means_B.shape[1]):\n",
        "  plt.fill_between(np.arange(len(smoothed_means_B)), (smoothed_means_B - smoothed_vars_B)[:, i], (smoothed_means_B + smoothed_vars_B)[:, i], alpha=0.1)\n",
        "plt.legend(np.arange(n_classes))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZdoFzJPHbzb"
      },
      "source": [
        "Here you see the weights assigned to the classes in domain A. As expected, 0 gets a large weight as it is underrepresented in domain A, and 1 gets a small weight as it is overrepresented in domain A."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUiituvrk-gs"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Assigned importances for the classes in domain A over the course of training')\n",
        "smoothed_importances_A = signal.savgol_filter(example_importances_A,101,3,axis=0)\n",
        "plt.plot(smoothed_importances_A)\n",
        "plt.legend(np.arange(n_classes))\n",
        "plt.ylabel('Assigned importance')\n",
        "plt.xlabel('Training iterations')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvvndeLhFnfl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}