{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance sampling: toy example with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for generation and visualization of the image batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_img_batch(batch):\n",
    "    '''Visualizes image batch'''\n",
    "    grid = make_grid(batch, nrow=8, padding=1, normalize=False, range=None, scale_each=False, pad_value=0)\n",
    "    plt.imshow(grid.permute(1,2,0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_image_data(size=(32768, 3, 1, 1), ratio=0.5):\n",
    "    '''Makes a random image batch of size (batch_size, height, width, channels) \n",
    "    with black to white ratio of value ratio\n",
    "    '''\n",
    "    idx = torch.randperm(size[0])[:int(ratio*size[0])]\n",
    "    image_batch = torch.zeros(size) + 0.2 # to make light gray\n",
    "    image_batch[idx] = 1 - 0.2 # to make light gray \n",
    "    return image_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackWhiteDataset(Dataset):\n",
    "    '''The dataloader for the black and white images'''\n",
    "    def __init__(self, weight_network):\n",
    "        self.dataset = random_image_data()\n",
    "        \n",
    "        self.weight_network = weight_network\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def accept_sample(self, weight_network, img):\n",
    "        # Returns True if the image is accepted, False if rejected\n",
    "        weight = weight_network(img)\n",
    "        return bool(list(torch.utils.data.sampler.WeightedRandomSampler([1-weight, weight], 1))[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Random permutation on the dataset order (is this equivalent to uniform sampling?)\n",
    "        all_idx = torch.randperm(len(dataset))\n",
    "        \n",
    "        # Loop through the samples and return once accepted\n",
    "        for i in all_idx:\n",
    "            accept = self.accept_sample(self.weight_network, self.dataset[i])\n",
    "            if accept:\n",
    "                return self.dataset[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight network with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 1)\n",
    "        self.fc2 = nn.Linear(1, 1)\n",
    "        \n",
    "        self.fc1.weight.data.fill_(0.5) # This as initialization because when the weights are too small\n",
    "        self.fc2.weight.data.fill_(0.5) # no images are sampled\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x.view(-1, 3)))\n",
    "        out = torch.sigmoid(self.fc2(h1))\n",
    "        return out\n",
    "\n",
    "weight_network = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BlackWhiteDataset(weight_network)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7313],\n",
      "        [0.8102]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "example_img = torch.cat((torch.Tensor([0.2, 0.2, 0.2]), torch.Tensor([0.8, 0.8, 0.8])))\n",
    "w = weight_network(example_img)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(weight_network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss:  0.13680525124073029\n",
      "step 50 loss:  0.08545710891485214\n",
      "step 100 loss:  0.052934058010578156\n",
      "step 150 loss:  0.014898096211254597\n",
      "step 200 loss:  0.00482331495732069\n",
      "step 250 loss:  0.002038559876382351\n",
      "step 300 loss:  0.0011581454891711473\n",
      "step 350 loss:  0.0003164297086186707\n",
      "step 400 loss:  0.0001686696632532403\n",
      "step 450 loss:  6.612746074097231e-05\n",
      "step 500 loss:  3.325877332827076e-05\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    labels = data.mean(1).view(-1, 1)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = weight_network(data)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    if i % 50 == 0:\n",
    "        print('step', i, 'loss: ', loss.item())\n",
    "#         print('outputs:', outputs[0].item(), 'labels:', labels[0].item())\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Helge's objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss:  19.333093643188477\n",
      "outputs: 0.20418091118335724 0.775668203830719 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 50 loss:  61.836910247802734\n",
      "outputs: 0.03269349783658981 0.03269349783658981 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 100 loss:  63.47189712524414\n",
      "outputs: 0.008053789846599102 0.008053789846599102 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 150 loss:  63.82422637939453\n",
      "outputs: 0.0026917282957583666 0.0026917282957583666 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 200 loss:  63.931575775146484\n",
      "outputs: 0.0010503757512196898 0.0010503757512196898 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 250 loss:  63.97072982788086\n",
      "outputs: 0.0004499885835684836 0.0004499885835684836 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 300 loss:  63.986663818359375\n",
      "outputs: 0.0002052288909908384 0.0002052288909908384 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 350 loss:  63.993648529052734\n",
      "outputs: 9.79064279817976e-05 9.79064279817976e-05 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 400 loss:  63.99686050415039\n",
      "outputs: 4.831937258131802e-05 4.831937258131802e-05 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 450 loss:  63.9984130859375\n",
      "outputs: 2.4488208509865217e-05 2.4488208509865217e-05 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 500 loss:  63.99917984008789\n",
      "outputs: 1.267806874238886e-05 1.267806874238886e-05 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "for i, data in enumerate(dataloader):\n",
    "#     labels = data.mean(1).view(-1, 1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = weight_network(data)\n",
    "    \n",
    "    # Ground truth:\n",
    "    ground_truth = data.mean(1).view(-1, 1) \n",
    "    \n",
    "    loss = ((1-outputs.detach()) * (outputs/outputs.detach()).view(-1, 1)).sum()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    if i % 50 == 0:\n",
    "        \n",
    "        w = weight_network(example_img)\n",
    "        print('step', i, 'loss: ', loss.item())\n",
    "        print('outputs:', w[0].item(), w[1].item(), 'ground_truth', example_img[0].item(), example_img[3].item())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_img_batch(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
