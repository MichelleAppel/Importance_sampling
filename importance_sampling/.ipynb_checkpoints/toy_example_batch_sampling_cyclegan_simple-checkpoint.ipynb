{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance sampling in cycleGAN: toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for generation and visualization of the image batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_img_batch(batch):\n",
    "    '''Visualizes image batch\n",
    "    \n",
    "    Parameters:\n",
    "    batch (Tensor): An image batch\n",
    "    '''\n",
    "    grid = make_grid(batch, nrow=8, padding=1, normalize=False, range=None, scale_each=False, pad_value=0)\n",
    "    plt.imshow(grid.permute(1,2,0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_image_data(size=(2**21, 3, 1, 1), ratio=0.5, offset=0.2, color=0):\n",
    "    '''Generates a random image batch \n",
    "    consisting of two modes (dark and light images)\n",
    "    \n",
    "    Parameters:\n",
    "    size (tuple): The dimensions of the image batch (batch_size, channels, width, length)\n",
    "    ratio (float): The ratio of light to dark images\n",
    "    offset (float): The brightness of the images relative to black and bright\n",
    "    color (int): Red = 0, green = 1, blue = 2\n",
    "    \n",
    "    Returns:\n",
    "    image_batch (Tensor): The generated image batch\n",
    "    \n",
    "    '''\n",
    "    idx = torch.randperm(size[0])[:int(ratio*size[0])] # Randomly choose indices according to the ratio\n",
    "    image_batch = torch.zeros(size)\n",
    "    image_batch[:, color] += offset # light color\n",
    "    image_batch[idx, color] = 1 - offset # dark color \n",
    "    return image_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorDataset(Dataset):\n",
    "    '''The dataloader for the color images\n",
    "    '''\n",
    "    def __init__(self, ratio=0.5, offset=0.2, color=0):\n",
    "        '''  \n",
    "        Parameters:\n",
    "        ratio (float): The ratio of light to dark images\n",
    "        offset (float): The brightness of the images relative to black and bright\n",
    "        color (int): Red = 0, green = 1, blue = 2\n",
    "        '''\n",
    "        self.offset = offset\n",
    "        self.ratio = ratio\n",
    "        self.color = color\n",
    "        \n",
    "        self.dataset = random_image_data(ratio=self.ratio, \n",
    "                                         offset=self.offset, \n",
    "                                         color=self.color)\n",
    "        self.example_imgs = self.example()\n",
    "        \n",
    "    def example(self):\n",
    "        '''\n",
    "        Returns an example from each mode in the domain\n",
    "        \n",
    "        '''\n",
    "        example_imgs = torch.zeros(size=(2, 3, 1, 1))\n",
    "\n",
    "        example_imgs[0, self.color] += self.offset # light color\n",
    "        example_imgs[1, self.color] = 1 - self.offset # dark color\n",
    "        return example_imgs   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):      \n",
    "        return self.dataset[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightNet(nn.Module):\n",
    "    '''A simple network that predicts the importances of the samples'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(WeightNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 1)\n",
    "        self.fc2 = nn.Linear(1, 1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = torch.sigmoid(self.fc1(x.view(-1, 3)))\n",
    "        out = self.softmax(self.fc2(h1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialWeightNet(nn.Module):\n",
    "    '''A trivial network that predicts the importances of the samples'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TrivialWeightNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.ones((x.size()[0], 1))\n",
    "        out /= out.sum()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''A simple joint discriminator network'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(6, 3)\n",
    "        self.fc2 = nn.Linear(3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = torch.sigmoid(self.fc1(x.view(-1, 6)))\n",
    "        out = torch.sigmoid(self.fc2(h1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialDiscriminator(nn.Module):\n",
    "    '''A trivial joint discriminator network'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TrivialDiscriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.zeros(x.size()[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''A simple conditional generator network'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 3)\n",
    "        self.fc2 = nn.Linear(3, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x.view(-1, 3)))\n",
    "        out = torch.sigmoid(self.fc2(h1))\n",
    "        return out.unsqueeze(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialGenerator(nn.Module):\n",
    "    '''A trivial conditional generator network'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TrivialGenerator, self).__init__()\n",
    "        self.nn = nn.Linear(1,1) # Otherwise error\n",
    "        \n",
    "    def forward(self, x):\n",
    "        new = torch.zeros((x.shape))\n",
    "        new[:, 0] = x[:, 1]\n",
    "        new[:, 1] = x[:, 0]\n",
    "        return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A demonstration of the two domains\n",
    "The idea here is that we create two domains: A red and a green domain. The domains consist of two modes: dark and light images. The big assumption here for our purpose is that the dark and light images have semantic correspondance, so for example: dark red will be translated to dark green.\n",
    "\n",
    "We can adjust the intensity of the images (offset) and the ratio of the modes for the experiment. In the visualiztion you can see the elements of the dataset. In the histogram you can see the modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAADsCAYAAAB39h09AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAMYklEQVR4nO3db6ie9X3H8fdnSWwHlulmqZKk6lgos4P1T8gsPhE3IToxg/kgwlpbOs4olVkobK6DlvWR24NuFKUSVKxb0Q4tXSYWceiwg+k8yeKfxEkzGZiQ4Wo2bWhR0n33IHd3Tk/vY87JfXmuo9/3C25yX/f18/79uIzvXPmdc25TVUiS3vl+buwFSJLWhsGXpCYMviQ1YfAlqQmDL0lNGHxJamKm4Cf5xSSPJPne5Ndzlxn34yQHJo+9s8wpSTozmeX78JP8BXC8qm5JcjNwblX98ZRxJ6rq7BnWKUma0azBfwG4vKqOJbkA+Meq+sCUcQZfkkY26x7++6rq2OT5fwLvW2bcu5PMJ3kiye/MOKck6QxsPN2AJP8AnD/l1J8uPqiqSrLcXxcurKqjSX4ZeDTJs1X171PmmgPmJocfPd3aJEk/4/tV9d5pJ9ZkS2fJP3M38GBV3X+acX7IjySt3r6q2j7txKxbOnuBGybPbwD+bumAJOcmedfk+XnAZcChGeeVJK3SrMG/BbgyyfeA35ock2R7kjsmY34VmE/yNPAYcEtVGXxJWmMzbem8ldzSkaQz8pZt6UiS3iYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYGCX6SnUleSHI4yc1Tzr8ryTcn559MctEQ80qSVm7m4CfZANwGXAVcAlyf5JIlwz4N/HdV/Qrwl8CfzzqvJGl1hrjD3wEcrqoXq+oN4D5g15Ixu4CvT57fD/xmkgwwtyRphYYI/mbgpUXHRyavTR1TVSeBV4FfWvpGSeaSzCeZH2BdkqRFNo69gMWqag+wByBJjbwcSXpHGeIO/yiwddHxlslrU8ck2Qj8AvDKAHNLklZoiOA/BWxLcnGSs4DdwN4lY/YCN0yeXwc8WlXewUvSGpp5S6eqTia5EXgY2ADcVVUHk3wZmK+qvcCdwF8nOQwc59QfCpKkNZT1eqPtHr4knZF9VbV92gl/0laSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITgwQ/yc4kLyQ5nOTmKec/meS/khyYPH5/iHklSSu3cdY3SLIBuA24EjgCPJVkb1UdWjL0m1V146zzSZLOzBB3+DuAw1X1YlW9AdwH7BrgfSVJAxoi+JuBlxYdH5m8ttTvJnkmyf1Jtg4wryRpFWbe0lmhvwfurarXk/wB8HXgiqWDkswBc2u0preNj469gHVk39gLWEfmx17AOrJ97AW8TQxxh38UWHzHvmXy2v+rqleq6vXJ4R0s07Cq2lNV26vKf3+SNLAhgv8UsC3JxUnOAnYDexcPSHLBosNrgecHmFeStAozb+lU1ckkNwIPAxuAu6rqYJIvA/NVtRf4wyTXAieB48AnZ51XkrQ6qaqx1zBVkvW5sBG4h7/APfwF7uEvcA/4p+xbblvcn7SVpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYGCX6Su5K8nOS5Zc4nyVeTHE7yTJKPDDGvJGnlhrrDvxvY+SbnrwK2TR5zwNcGmleStEKDBL+qHgeOv8mQXcA9dcoTwDlJLhhibknSyqzVHv5m4KVFx0cmr/2UJHNJ5pPMr9G6JKmNjWMvYLGq2gPsAUhSIy9Hkt5R1uoO/yiwddHxlslrkqQ1slbB3wt8YvLdOpcCr1bVsTWaW5LEQFs6Se4FLgfOS3IE+BKwCaCqbgceAq4GDgM/BD41xLySpJUbJPhVdf1pzhfw2SHmkiSdGX/SVpKaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhODBD/JXUleTvLcMucvT/JqkgOTxxeHmFeStHIbB3qfu4FbgXveZMx3q+qageaTJK3SIHf4VfU4cHyI95IkvTXWcg//Y0meTvKdJB9cw3klSQy3pXM6+4ELq+pEkquBbwPblg5KMgfMrdGa3jb2jb0ArUvbx16A3nbW5A6/ql6rqhOT5w8Bm5KcN2XcnqraXlX+Xpakga1J8JOcnyST5zsm876yFnNLkk4ZZEsnyb3A5cB5SY4AXwI2AVTV7cB1wGeSnAR+BOyuqhpibknSymS9djfJ+lyYJK1v+5bbFvcnbSWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpiZmDn2RrkseSHEpyMMlNU8YkyVeTHE7yTJKPzDqvJGl1Ng7wHieBz1fV/iTvAfYleaSqDi0acxWwbfL4DeBrk18lSWtk5jv8qjpWVfsnz38APA9sXjJsF3BPnfIEcE6SC2adW5K0coPu4Se5CPgw8OSSU5uBlxYdH+Fn/1AgyVyS+STzQ65LkjTMlg4ASc4GHgA+V1Wvncl7VNUeYM/k/WqotUmSBrrDT7KJU7H/RlV9a8qQo8DWRcdbJq9JktbIEN+lE+BO4Pmq+soyw/YCn5h8t86lwKtVdWzWuSVJKzfEls5lwMeBZ5McmLz2BeD9AFV1O/AQcDVwGPgh8KkB5pUkrUKq1udWuXv4knRG9lXV9mkn/ElbSWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTcwc/CRbkzyW5FCSg0lumjLm8iSvJjkweXxx1nklSauzcYD3OAl8vqr2J3kPsC/JI1V1aMm471bVNQPMJ0k6AzPf4VfVsaraP3n+A+B5YPOs7ytJGtage/hJLgI+DDw55fTHkjyd5DtJPjjkvJKk0xtiSweAJGcDDwCfq6rXlpzeD1xYVSeSXA18G9g25T3mgLnJ4QnghaHWN4PzgO+PvYh1wmuxwGuxwGuxYD1ciwuXO5Gqmvndk2wCHgQerqqvrGD8fwDbq2rsC3NaSearavvY61gPvBYLvBYLvBYL1vu1GOK7dALcCTy/XOyTnD8ZR5Idk3lfmXVuSdLKDbGlcxnwceDZJAcmr30BeD9AVd0OXAd8JslJ4EfA7hrirxaSpBWbOfhV9U9ATjPmVuDWWecayZ6xF7COeC0WeC0WeC0WrOtrMcgeviRp/fOjFSSpCYO/jCQ7k7yQ5HCSm8dez5iS3JXk5STPjb2WMa3kY0S6SPLuJP8y+dmag0n+bOw1jS3JhiT/muTBsdeyHIM/RZINwG3AVcAlwPVJLhl3VaO6G9g59iLWgZ98jMglwKXAZxv/vngduKKqfh34ELAzyaUjr2lsN3HqkwbWLYM/3Q7gcFW9WFVvAPcBu0Ze02iq6nHg+NjrGJsfI7KgTjkxOdw0ebT9gmCSLcBvA3eMvZY3Y/Cn2wy8tOj4CE3/w9Z0p/kYkRYmWxgHgJeBR6qq7bUA/gr4I+B/x17ImzH40iqd5mNE2qiqH1fVh4AtwI4kvzb2msaQ5Brg5araN/ZaTsfgT3cU2LroeMvkNTU3+RiRB4BvVNW3xl7PelBV/wM8Rt+v81wGXDv5yJj7gCuS/M24S5rO4E/3FLAtycVJzgJ2A3tHXpNGtpKPEekiyXuTnDN5/vPAlcC/jbuqcVTVn1TVlqq6iFOteLSqfm/kZU1l8KeoqpPAjcDDnPrC3N9W1cFxVzWeJPcC/wx8IMmRJJ8ee00j+cnHiFyx6P/edvXYixrJBcBjSZ7h1A3SI1W1br8dUaf4k7aS1IR3+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+Smvg/uXwKFA11sm0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Settings for domain A (red)\n",
    "offset_A = 0.2\n",
    "ratio_A = 0.5\n",
    "color_A = 0\n",
    "dataset_A = ColorDataset(offset=offset_A, ratio=ratio_A, color=color_A)\n",
    "dataloader_A = DataLoader(dataset_A, batch_size=256, shuffle=True)\n",
    "\n",
    "# Settings for domain B (green)\n",
    "offset_B = 0.2\n",
    "ratio_B = 0.5\n",
    "color_B = 1\n",
    "dataset_B = ColorDataset(offset=offset_B, ratio=ratio_B, color=color_B)\n",
    "dataloader_B = DataLoader(dataset_B, batch_size=256, shuffle=True)\n",
    "\n",
    "visualize_img_batch(dataset_A.example_imgs)\n",
    "visualize_img_batch(dataset_B.example_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ratio domain A: 0.5\n",
      "Count: tensor([122, 134]) Ratio in this batch: tensor([0.4766, 0.5234])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ratio domain B: 0.5\n",
      "Count: tensor([125, 131]) Ratio in this batch: tensor([0.4883, 0.5117])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAANyElEQVR4nO3cYWzc913H8feniQJidAOtRpqSdAkj0zCjYpvJ9ggGFJR2IhnqQIlUaZE6ok0Lm9Q9WKZOFcoesLZSJyTyYGFUVEglLX3kqSnR2FqhTWSKS7NWaZXhhkASJOZ1ZROa1i7w5YGv4/Bs39+5s+/64/2SLN3/7iff95dz3jr/z3epKiRJr33XjXsASdJoGHRJaoRBl6RGGHRJaoRBl6RGbB7XHd9www21Y8eOcd29JL0mPfXUU9+uqqnlbhtb0Hfs2MHc3Ny47l6SXpOS/MtKt3nKRZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaMbZ3ikrSOO048tjY7vviZ9+3Lt+30zP0JHuSnE8yn+TIMrcfTLKQ5Gzv60OjH1WStJqBz9CTbAKOAb8NXAbOJJmtqueWLH24qg6vw4ySpA66PEPfDcxX1YWqegU4Aexb37EkSWvVJehbgUt9x5d71y11W5JnkjyaZPty3yjJoSRzSeYWFhauYVxJ0kpG9VcuXwR2VNVNwJeAB5dbVFXHq2qmqmamppb9OF9J0jXqEvQrQP8z7m29636kql6sqpd7h18A3jWa8SRJXXUJ+hlgV5KdSbYA+4HZ/gVJ3tR3uBd4fnQjSpK6GPhXLlV1Nclh4BSwCXigqs4lOQrMVdUs8LEke4GrwHeAg+s4syRpGZ3eWFRVJ4GTS667u+/yp4BPjXa0lbX4hgBJGpZv/ZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWpEp6An2ZPkfJL5JEdWWXdbkkoyM7oRJUldDAx6kk3AMeAWYBo4kGR6mXXXAx8Hvj7qISVJg3V5hr4bmK+qC1X1CnAC2LfMus8A9wA/GOF8kqSOugR9K3Cp7/hy77ofSfJOYHtVPbbaN0pyKMlckrmFhYU1DytJWtnQL4omuQ64H/jEoLVVdbyqZqpqZmpqati7liT16RL0K8D2vuNtvetedT3wduDJJBeB9wCzvjAqSRurS9DPALuS7EyyBdgPzL56Y1V9t6puqKodVbUDOA3sraq5dZlYkrSsgUGvqqvAYeAU8DzwSFWdS3I0yd71HlCS1M3mLouq6iRwcsl1d6+w9r3DjyVJWivfKSpJjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjegU9CR7kpxPMp/kyDK3fzjJs0nOJvlqkunRjypJWs3AoCfZBBwDbgGmgQPLBPuhqvrlqvoV4F7g/pFPKklaVZdn6LuB+aq6UFWvACeAff0Lqup7fYevA2p0I0qSutjcYc1W4FLf8WXg3UsXJfkocCewBfjN5b5RkkPAIYAbb7xxrbNKklYxshdFq+pYVb0F+CTw6RXWHK+qmaqamZqaGtVdS5LoFvQrwPa+422961ZyAnj/MENJktauS9DPALuS7EyyBdgPzPYvSLKr7/B9wD+NbkRJUhcDz6FX1dUkh4FTwCbggao6l+QoMFdVs8DhJDcDPwReAj64nkNLkn5clxdFqaqTwMkl193dd/njI55LkrRGvlNUkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEZ2CnmRPkvNJ5pMcWeb2O5M8l+SZJF9O8ubRjypJWs3AoCfZBBwDbgGmgQNJppcsexqYqaqbgEeBe0c9qCRpdV2eoe8G5qvqQlW9ApwA9vUvqKonqur7vcPTwLbRjilJGqRL0LcCl/qOL/euW8kdwOPL3ZDkUJK5JHMLCwvdp5QkDTTSF0WT3A7MAPctd3tVHa+qmaqamZqaGuVdS9L/e5s7rLkCbO873ta77v9IcjNwF/DrVfXyaMaTJHXV5Rn6GWBXkp1JtgD7gdn+BUneAXwe2FtV3xr9mJKkQQYGvaquAoeBU8DzwCNVdS7J0SR7e8vuA34a+JskZ5PMrvDtJEnrpMspF6rqJHByyXV3912+ecRzSZLWyHeKSlIjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjOgU9yZ4k55PMJzmyzO2/luQfk1xN8oHRjylJGmRg0JNsAo4BtwDTwIEk00uW/StwEHho1ANKkrrZ3GHNbmC+qi4AJDkB7AOee3VBVV3s3fbf6zCjJKmDLqdctgKX+o4v965bsySHkswlmVtYWLiWbyFJWsGGvihaVceraqaqZqampjbyriWpeV2CfgXY3ne8rXedJGmCdAn6GWBXkp1JtgD7gdn1HUuStFYDg15VV4HDwCngeeCRqjqX5GiSvQBJfjXJZeD3gc8nObeeQ0uSflyXv3Khqk4CJ5dcd3ff5TMsnoqRJI2J7xSVpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqRKegJ9mT5HyS+SRHlrn9J5I83Lv960l2jHpQSdLqBgY9ySbgGHALMA0cSDK9ZNkdwEtV9QvA54B7Rj2oJGl1XZ6h7wbmq+pCVb0CnAD2LVmzD3iwd/lR4LeSZHRjSpIG2dxhzVbgUt/xZeDdK62pqqtJvgu8Efh2/6Ikh4BDvcP/THL+WoYGblj6vTdKRv+7x9j2sg5a2Usr+wD3MpFyz1B7efNKN3QJ+shU1XHg+LDfJ8lcVc2MYKSxcy+Tp5V9gHuZVOu1ly6nXK4A2/uOt/WuW3ZNks3AG4AXRzGgJKmbLkE/A+xKsjPJFmA/MLtkzSzwwd7lDwBfqaoa3ZiSpEEGnnLpnRM/DJwCNgEPVNW5JEeBuaqaBf4C+Ksk88B3WIz+ehr6tM0EcS+Tp5V9gHuZVOuyl/hEWpLa4DtFJakRBl2SGjHRQe/wkQN3JnkuyTNJvpxkxb/PHLcOe/lwkmeTnE3y1WXejTsRBu2jb91tSSrJxP6ZWYfH5GCShd5jcjbJh8YxZxddHpckf9D7/3IuyUMbPWNXHR6Xz/U9Jt9M8h/jmHOQDvu4MckTSZ7uNezWoe+0qibyi8UXYF8Afh7YAnwDmF6y5jeAn+pd/gjw8LjnHmIvr++7vBf423HPfS376K27Hvh74DQwM+65h3hMDgJ/Nu5ZR7SXXcDTwM/2jn9u3HMP8zPWt/6PWPxDjbHPfg2PyXHgI73L08DFYe93kp+hD/zIgap6oqq+3zs8zeLfyE+iLnv5Xt/h64BJfLW6y8dAAHyGxc/z+cFGDrdGXffyWtBlL38IHKuqlwCq6lsbPGNXa31cDgB/vSGTrU2XfRTw+t7lNwD/NuydTnLQl/vIga2rrL8DeHxdJ7p2nfaS5KNJXgDuBT62QbOtxcB9JHknsL2qHtvIwa5B15+v23q/Dj+aZPsyt0+CLnt5K/DWJF9LcjrJng2bbm06/7/vnWLdCXxlA+Zaqy77+GPg9iSXgZMs/rYxlEkOemdJbgdmgPvGPcswqupYVb0F+CTw6XHPs1ZJrgPuBz4x7llG5IvAjqq6CfgS//sBdK9Fm1k87fJeFp/V/nmSnxnrRMPbDzxaVf817kGu0QHgL6tqG3Ari+/lGarJkxz0Lh85QJKbgbuAvVX18gbNtlad9tLnBPD+dZ3o2gzax/XA24Enk1wE3gPMTugLowMfk6p6se9n6gvAuzZotrXq8vN1GZitqh9W1T8D32Qx8JNmLf9X9jOZp1ug2z7uAB4BqKp/AH6SxQ8gu3bjfvFglRcVNgMXWPyV6tUXFX5pyZp3sPjCw65xzzuCvezqu/y7LL4Ld+yzr3UfS9Y/yeS+KNrlMXlT3+XfA06Pe+4h9rIHeLB3+QYWTwe8cdyzX+vPGPA24CK9N0dO2lfHx+Rx4GDv8i+yeA59qP2MfeMD/lFuZfGZxAvAXb3rjrL4bBzg74B/B872vmbHPfMQe/lT4FxvH0+sFspJ3seStRMb9I6PyZ/0HpNv9B6Tt4175iH2EhZPhz0HPAvsH/fMw/yMsXj++bPjnnXIx2Qa+Frv5+ss8DvD3qdv/ZekRkzyOXRJ0hoYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEb8D5RtKPVcWz5MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_hist(data):\n",
    "    data = data.view(3, -1).sum(0)\n",
    "    plt.hist(data, weights=torch.ones(len(data))/len(data))\n",
    "    x_unique_count = torch.stack([(data==x_u).sum() for x_u in data.unique()])\n",
    "    \n",
    "    print('Count:', x_unique_count, 'Ratio in this batch:', x_unique_count/float(x_unique_count.sum()))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for i, (data_A, data_B) in enumerate(zip(dataloader_A, dataloader_B)):\n",
    "    print('True ratio domain A:', dataloader_A.dataset.ratio)\n",
    "    plot_hist(data_A)\n",
    "    print('True ratio domain B:', dataloader_B.dataset.ratio)\n",
    "    plot_hist(data_B)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss_g:  1.0672811269760132 loss_d:  0.5524616241455078 cycle 0.41078853607177734 loss_w:  129.95401000976562\n",
      "step 500 loss_g:  0.12977558374404907 loss_d:  0.6221374869346619 cycle 0.003559708595275879 loss_w:  48.350257873535156\n",
      "step 1000 loss_g:  0.12505006790161133 loss_d:  0.6237872242927551 cycle 0.0004610020259860903 loss_w:  47.95509338378906\n",
      "step 1500 loss_g:  0.1339845061302185 loss_d:  0.6243939399719238 cycle 0.00626456830650568 loss_w:  48.93714904785156\n",
      "step 2000 loss_g:  0.13616475462913513 loss_d:  0.6029508113861084 cycle 0.0016519075725227594 loss_w:  47.40911865234375\n",
      "step 2500 loss_g:  0.14923594892024994 loss_d:  0.5758908987045288 cycle 0.001312381587922573 loss_w:  46.49211120605469\n",
      "step 3000 loss_g:  0.11611443012952805 loss_d:  0.6430347561836243 cycle 0.0052629755809903145 loss_w:  48.92237854003906\n",
      "step 3500 loss_g:  0.13656741380691528 loss_d:  0.5972625017166138 cycle 0.00023240584414452314 loss_w:  46.97999572753906\n",
      "step 4000 loss_g:  0.12978625297546387 loss_d:  0.6416435837745667 cycle 0.0007325336337089539 loss_w:  49.418392181396484\n",
      "step 4500 loss_g:  0.11518844962120056 loss_d:  0.6735248565673828 cycle 0.014066282659769058 loss_w:  51.377891540527344\n",
      "step 5000 loss_g:  0.12424661219120026 loss_d:  0.6278749704360962 cycle 3.9828464650781825e-05 loss_w:  48.138328552246094\n",
      "step 5500 loss_g:  0.12492349743843079 loss_d:  0.6249500513076782 cycle 4.159531454206444e-05 loss_w:  47.994571685791016\n",
      "step 6000 loss_g:  0.1251296103000641 loss_d:  0.6252066493034363 cycle 3.7150261050555855e-05 loss_w:  48.02389907836914\n"
     ]
    }
   ],
   "source": [
    "# Initialize the networks\n",
    "weight_network_A = WeightNet()\n",
    "weight_network_B = WeightNet()\n",
    "generator_A = Generator()\n",
    "generator_B = Generator()\n",
    "discriminator_A = Discriminator()\n",
    "discriminator_B = Discriminator()\n",
    "\n",
    "# Initialize the optimizers\n",
    "optimizer_w = optim.Adam(itertools.chain(weight_network_A.parameters(), \n",
    "                                         weight_network_B.parameters()), lr=0.01)\n",
    "optimizer_g = optim.Adam(itertools.chain(generator_A.parameters(),\n",
    "                                         generator_B.parameters()), lr=0.01)\n",
    "optimizer_d = optim.Adam(itertools.chain(discriminator_A.parameters(),\n",
    "                                         discriminator_B.parameters()), lr=0.01)\n",
    "\n",
    "# Store values\n",
    "samples_A = []\n",
    "samples_B = []\n",
    "\n",
    "losses_g = []\n",
    "losses_d = []\n",
    "losses_w = []\n",
    "\n",
    "Lminusses = []\n",
    "Lplusses = []\n",
    "\n",
    "example_importances_A = []\n",
    "example_importances_B = []\n",
    "\n",
    "criterion_D = nn.MSELoss()\n",
    "criterion_G = nn.MSELoss()\n",
    "criterion_cycle = nn.MSELoss()\n",
    "criterion_L2 = nn.MSELoss()\n",
    "\n",
    "sampled_batch_size = 64 # The amount of images sampled using importance sampling\n",
    "for epoch in range(5):\n",
    "    for i, (data_A, data_B) in enumerate(zip(dataloader_A, dataloader_B)):\n",
    "        # Set gradients to zero\n",
    "        optimizer_w.zero_grad()\n",
    "        optimizer_d.zero_grad()\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        # The sampling process ----------------------------------------------------------------------------\n",
    "        importances_A = weight_network_A(data_A).squeeze() # Get the importances for each image in domain A\n",
    "        importances_B = weight_network_B(data_B).squeeze() # Get the importances for each image in domain B\n",
    "\n",
    "        sampled_idx_A = list( # Sample from batch A according to these importances\n",
    "            torch.utils.data.sampler.WeightedRandomSampler(importances_A, \n",
    "                                                           sampled_batch_size, \n",
    "                                                           replacement=False))\n",
    "\n",
    "        sampled_importances_A = importances_A[sampled_idx_A] # The importances assigned to the smaller batch\n",
    "        real_A = data_A[sampled_idx_A] # The sampled smaller batch A\n",
    "\n",
    "        sampled_idx_B = list( # Sample from batch Baccording to these importances\n",
    "            torch.utils.data.sampler.WeightedRandomSampler(importances_B,\n",
    "                                                           sampled_batch_size, \n",
    "                                                           replacement=False))\n",
    "\n",
    "        sampled_importances_B = importances_B[sampled_idx_B] # The importances assigned to the smaller batch\n",
    "        real_B = data_B[sampled_idx_B] # The sampled smaller batch B\n",
    "        # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Forward\n",
    "        fake_A = generator_A(real_B)\n",
    "        fake_B = generator_B(real_A)\n",
    "        \n",
    "        rec_A = generator_A(fake_B)\n",
    "        rec_B = generator_B(fake_A)\n",
    "        \n",
    "        pred_real_A = discriminator_A(real_A)\n",
    "        pred_real_B = discriminator_B(real_B)\n",
    "        \n",
    "        pred_fake_A = discriminator_A(fake_A.detach())\n",
    "        pred_fake_B = discriminator_B(fake_B.detach())\n",
    "\n",
    "        # The loss function --------------------------------------------------------------------------------\n",
    "        loss_D_real_A = criterion_D(pred_real_A, torch.ones((pred_real_A.shape)))\n",
    "        loss_D_real_B = criterion_D(pred_real_B, torch.ones((pred_real_B.shape)))\n",
    "\n",
    "        loss_D_fake_A = criterion_D(pred_fake_A, torch.zeros((pred_fake_A.shape)))\n",
    "        loss_D_fake_B = criterion_D(pred_fake_B, torch.zeros((pred_fake_B.shape)))\n",
    "        \n",
    "        loss_D_A = (loss_D_real_A + loss_D_fake_A) * 0.5\n",
    "        loss_D_B = (loss_D_real_B + loss_D_fake_B) * 0.5\n",
    "        \n",
    "        loss_G_A = criterion_G(discriminator_A(fake_A), torch.ones((pred_fake_A.shape)))\n",
    "        loss_G_B = criterion_G(discriminator_B(fake_B), torch.ones((pred_fake_A.shape)))\n",
    "        \n",
    "        cycle_A = criterion_cycle(real_A, rec_A)\n",
    "        cycle_B = criterion_cycle(real_B, rec_B)\n",
    "        cycle = cycle_A + cycle_B\n",
    "        \n",
    "        loss_d = (loss_D_A + loss_D_B)\n",
    "        loss_g = (loss_G_A + loss_G_B) + cycle\n",
    "        loss_w = ((loss_G_A.detach() + loss_D_A.detach() + cycle.detach()) * sampled_importances_A/sampled_importances_A.detach()).sum() + \\\n",
    "                 ((loss_G_B.detach() + loss_D_B.detach() + cycle.detach()) * sampled_importances_B/sampled_importances_B.detach()).sum()\n",
    "        # ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Backward\n",
    "        loss_g.backward()\n",
    "        loss_d.backward()\n",
    "        loss_w.backward()\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer_g.step()\n",
    "        optimizer_w.step()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Store values --------------------------------------------------------------------------------------\n",
    "        losses_g += [loss_g.item()]\n",
    "        losses_d += [loss_d.item()]\n",
    "        losses_w += [loss_w.item()]\n",
    "\n",
    "        w_a = weight_network_A(dataset_A.example_imgs)\n",
    "        w_b = weight_network_B(dataset_B.example_imgs)\n",
    "        example_importances_A += [(w_a[0].item(), w_a[1].item())] # Store examples in a list\n",
    "        example_importances_B += [(w_b[0].item(), w_b[1].item())] # Store examples in a list\n",
    "        # ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Print statistics\n",
    "        if i % 500 == 0:\n",
    "            samples_A += [fake_A.detach()]\n",
    "            samples_B += [fake_B.detach()]\n",
    "            print('step', i, 'loss_g: ', loss_g.item(), 'loss_d: ', loss_d.item(), 'cycle', cycle.item(),'loss_w: ', loss_w.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_img_batch(real_A)\n",
    "visualize_img_batch(fake_B.detach())\n",
    "print('True ratio domain A {}'.format(ratio_A))\n",
    "plot_hist(real_A)\n",
    "\n",
    "visualize_img_batch(real_B)\n",
    "visualize_img_batch(fake_A.detach())\n",
    "print('True ratio domain B {}'.format(ratio_B))\n",
    "plot_hist(real_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Losses over iterations')\n",
    "plt.xlabel('Training iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(losses_g)\n",
    "plt.plot(losses_d)\n",
    "plt.plot(losses_w)\n",
    "plt.legend(['G', 'D', 'W'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Assigned importances for the toy example images over the course of training')\n",
    "plt.plot(example_importances_A)\n",
    "plt.plot(example_importances_B)\n",
    "plt.legend(['Img A with value {} (p={})'.format(offset_A, ratio_A), \n",
    "            'Img A with value {} (p={})'.format(1-offset_A, 1-ratio_A), \n",
    "            'Img B with value {} (p={})'.format(offset_B, ratio_B), \n",
    "            'Img B with value {} (p={})'.format(1-offset_B, 1-ratio_B)])\n",
    "plt.xlabel('Assigned importance')\n",
    "plt.ylabel('Training iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Ratio between importances of the same mode')\n",
    "plt.plot(torch.Tensor(example_importances_A)[:, 0]/torch.Tensor(example_importances_B)[:, 0])\n",
    "plt.plot(torch.Tensor(example_importances_A)[:, 1]/torch.Tensor(example_importances_B)[:, 1])\n",
    "plt.legend(['Dark images', 'Light images'])\n",
    "plt.xlabel('Ratio importance')\n",
    "plt.ylabel('Training iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in samples_B:\n",
    "    visualize_img_batch(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
