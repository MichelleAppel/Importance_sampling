{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance sampling: toy example with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for generation and visualization of the image batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_img_batch(batch):\n",
    "    '''Visualizes image batch'''\n",
    "    grid = make_grid(batch, nrow=8, padding=1, normalize=False, range=None, scale_each=False, pad_value=0)\n",
    "    plt.imshow(grid.permute(1,2,0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_image_data(size=(32768, 3, 1, 1), ratio=0.5):\n",
    "    '''Makes a random image batch of size (batch_size, height, width, channels) \n",
    "    with black to white ratio of value ratio\n",
    "    '''\n",
    "    idx = torch.randperm(size[0])[:int(ratio*size[0])]\n",
    "    image_batch = torch.zeros(size) + 0.2 # to make light gray\n",
    "    image_batch[idx] = 1 - 0.2 # to make light gray \n",
    "    return image_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackWhiteDataset(Dataset):\n",
    "    '''The dataloader for the black and white images'''\n",
    "    def __init__(self, weight_network):\n",
    "        self.dataset = random_image_data()\n",
    "        \n",
    "        self.weight_network = weight_network\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def accept_sample(self, weight_network, img):\n",
    "        # Returns True if the image is accepted, False if rejected\n",
    "        weight = weight_network(img)\n",
    "        return bool(list(torch.utils.data.sampler.WeightedRandomSampler([1-weight, weight], 1))[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Random permutation on the dataset order (is this equivalent to uniform sampling?)\n",
    "        all_idx = torch.randperm(len(dataset))\n",
    "        \n",
    "        # Loop through the samples and return once accepted\n",
    "        for i in all_idx:\n",
    "            accept = self.accept_sample(self.weight_network, self.dataset[i])\n",
    "            if accept:\n",
    "                return self.dataset[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight network with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 1)\n",
    "        self.fc2 = nn.Linear(1, 1)\n",
    "        \n",
    "        self.fc1.weight.data.fill_(0.5) # This as initialization because when the weights are too small\n",
    "        self.fc2.weight.data.fill_(0.5) # no images are sampled\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = torch.sigmoid(self.fc1(x.view(-1, 3)))\n",
    "        out = torch.sigmoid(self.fc2(h1))\n",
    "        return out\n",
    "\n",
    "weight_network = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BlackWhiteDataset(weight_network)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4107],\n",
      "        [0.4367]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "example_img = torch.cat((torch.Tensor([0.2, 0.2, 0.2]), torch.Tensor([0.8, 0.8, 0.8])))\n",
    "w = weight_network(example_img)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(weight_network.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(dataloader):\n",
    "#     labels = data.mean(1).view(-1, 1)\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # forward + backward + optimize\n",
    "#     outputs = weight_network(data)\n",
    "#     loss = criterion(outputs, labels)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # print statistics\n",
    "#     if i % 50 == 0:\n",
    "#         print('step', i, 'loss: ', loss.item())\n",
    "# #         print('outputs:', outputs[0].item(), 'labels:', labels[0].item())\n",
    "# #         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Helge's objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss:  -1.2000000476837158\n",
      "outputs: 0.7748978137969971 0.7909761071205139 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 50 loss:  -1.8000000715255737\n",
      "outputs: 0.7748932838439941 0.7909656167030334 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 100 loss:  0.6000000238418579\n",
      "outputs: 0.7748880386352539 0.7909545302391052 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 150 loss:  1.2000000476837158\n",
      "outputs: 0.7748895883560181 0.7909502387046814 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 200 loss:  3.0\n",
      "outputs: 0.7748900651931763 0.7909450531005859 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 250 loss:  1.2000000476837158\n",
      "outputs: 0.7748743295669556 0.7909229397773743 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 300 loss:  -0.6000000238418579\n",
      "outputs: 0.774872899055481 0.7909157276153564 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 350 loss:  3.5999999046325684\n",
      "outputs: 0.7748581171035767 0.790894627571106 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 400 loss:  7.199999809265137\n",
      "outputs: 0.7748394012451172 0.7908695936203003 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 450 loss:  1.1999999284744263\n",
      "outputs: 0.7748326063156128 0.790856659412384 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 500 loss:  2.4000000953674316\n",
      "outputs: 0.7748292684555054 0.7908474802970886 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "for i, data in enumerate(dataloader):\n",
    "#     labels = data.mean(1).view(-1, 1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = weight_network(data)\n",
    "    \n",
    "    # Ground truth:\n",
    "    ground_truth = data.mean(1).view(-1, 1) \n",
    "    \n",
    "    loss = ( (data.mean(1).view(-1, 1)-0.5) * (outputs/outputs.detach()).view(-1, 1)).sum()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    if i % 50 == 0:\n",
    "        \n",
    "        w = weight_network(example_img)\n",
    "        print('step', i, 'loss: ', loss.item())\n",
    "        print('outputs:', w[0].item(), w[1].item(), 'ground_truth', example_img[0].item(), example_img[3].item())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "light grey = 0.2, dark grey = 0.8\n"
     ]
    }
   ],
   "source": [
    "visualize_img_batch(data)\n",
    "print('light grey = 0.2, dark grey = 0.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
