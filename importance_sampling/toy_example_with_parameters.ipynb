{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance sampling: toy example with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for generation and visualization of the image batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_img_batch(batch):\n",
    "    '''Visualizes image batch'''\n",
    "    grid = make_grid(batch, nrow=8, padding=1, normalize=False, range=None, scale_each=False, pad_value=0)\n",
    "    plt.imshow(grid.permute(1,2,0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_image_data(size=(32768, 3, 1, 1), ratio=0.5):\n",
    "    '''Makes a random image batch of size (batch_size, height, width, channels) \n",
    "    with black to white ratio of value ratio\n",
    "    '''\n",
    "    idx = torch.randperm(size[0])[:int(ratio*size[0])]\n",
    "    image_batch = torch.zeros(size) + 0.2 # to make light gray\n",
    "    image_batch[idx] = 1 - 0.2 # to make light gray \n",
    "    return image_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackWhiteDataset(Dataset):\n",
    "    '''The dataloader for the black and white images'''\n",
    "    def __init__(self, weight_network):\n",
    "        self.dataset = random_image_data()\n",
    "        \n",
    "        self.weight_network = weight_network\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def accept_sample(self, weight_network, img):\n",
    "        # Returns True if the image is accepted, False if rejected\n",
    "        weight = weight_network(img)\n",
    "        return bool(list(torch.utils.data.sampler.WeightedRandomSampler([1-weight, weight], 1))[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Random permutation on the dataset order (is this equivalent to uniform sampling?)\n",
    "        all_idx = torch.randperm(len(dataset))\n",
    "        \n",
    "        # Loop through the samples and return once accepted\n",
    "        for i in all_idx:\n",
    "            accept = self.accept_sample(self.weight_network, self.dataset[i])\n",
    "            if accept:\n",
    "                return self.dataset[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight network with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 1)\n",
    "        self.fc2 = nn.Linear(1, 1)\n",
    "        \n",
    "        self.fc1.weight.data.fill_(0.5) # This as initialization because when the weights are too small\n",
    "        self.fc2.weight.data.fill_(0.5) # no images are sampled\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = torch.sigmoid(self.fc1(x.view(-1, 3)))\n",
    "        out = torch.sigmoid(self.fc2(h1))\n",
    "        return out\n",
    "\n",
    "weight_network = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BlackWhiteDataset(weight_network)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5478],\n",
      "        [0.5723]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "example_img = torch.cat((torch.Tensor([0.2, 0.2, 0.2]), torch.Tensor([0.8, 0.8, 0.8])))\n",
    "w = weight_network(example_img)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(weight_network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(dataloader):\n",
    "#     labels = data.mean(1).view(-1, 1)\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # forward + backward + optimize\n",
    "#     outputs = weight_network(data)\n",
    "#     loss = criterion(outputs, labels)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # print statistics\n",
    "#     if i % 50 == 0:\n",
    "#         print('step', i, 'loss: ', loss.item())\n",
    "# #         print('outputs:', outputs[0].item(), 'labels:', labels[0].item())\n",
    "# #         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Helge's objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss:  1.0\n",
      "outputs: 0.5991660356521606 0.6218259334564209 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 50 loss:  1.0\n",
      "outputs: 0.5258018970489502 0.5425145030021667 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 100 loss:  1.0\n",
      "outputs: 0.43836143612861633 0.4471900761127472 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 150 loss:  1.0\n",
      "outputs: 0.33809298276901245 0.3371690511703491 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 200 loss:  1.0\n",
      "outputs: 0.2370474934577942 0.22805234789848328 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 250 loss:  1.0\n",
      "outputs: 0.15022806823253632 0.1376960724592209 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 300 loss:  1.0\n",
      "outputs: 0.0856408104300499 0.07386159151792526 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 350 loss:  1.0\n",
      "outputs: 0.044076886028051376 0.03535878658294678 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 400 loss:  1.0\n",
      "outputs: 0.02083910070359707 0.015484562143683434 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 450 loss:  1.0\n",
      "outputs: 0.009285617619752884 0.006481642834842205 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 500 loss:  1.0\n",
      "outputs: 0.003911550622433424 0.002591117285192013 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 550 loss:  1.0\n",
      "outputs: 0.0015825364971533418 0.0010118724312633276 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 600 loss:  1.0\n",
      "outputs: 0.0006199689232744277 0.0003896499110851437 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 650 loss:  1.0\n",
      "outputs: 0.00023761737975291908 0.00014823059609625489 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 700 loss:  1.0\n",
      "outputs: 8.981490100268275e-05 5.588508065557107e-05 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 750 loss:  1.0\n",
      "outputs: 3.340816329000518e-05 2.104251143464353e-05 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 800 loss:  1.0\n",
      "outputs: 1.2287318895687349e-05 7.91182083048625e-06 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 850 loss:  1.0\n",
      "outputs: 4.535893367574317e-06 2.957038304884918e-06 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 900 loss:  1.0\n",
      "outputs: 1.6685203263477888e-06 1.102970372812706e-06 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 950 loss:  1.0\n",
      "outputs: 6.118710871305666e-07 4.1090765989793e-07 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1000 loss:  1.0\n",
      "outputs: 2.2526495513375266e-07 1.5263471198068146e-07 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1050 loss:  1.0\n",
      "outputs: 8.236593629362687e-08 5.6719258623161295e-08 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1100 loss:  1.0\n",
      "outputs: 3.019003713689017e-08 2.1039332054328952e-08 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1150 loss:  1.0\n",
      "outputs: 1.0993201016162857e-08 7.811749469510687e-09 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1200 loss:  1.0\n",
      "outputs: 4.007576492881526e-09 2.898133955042681e-09 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1250 loss:  1.0\n",
      "outputs: 1.4750194399226757e-09 1.0717136067484034e-09 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1300 loss:  1.0\n",
      "outputs: 5.397112756710953e-10 3.966902639707115e-10 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1350 loss:  1.0\n",
      "outputs: 1.9711668408639582e-10 1.468416360728142e-10 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1400 loss:  1.0\n",
      "outputs: 7.182033440189528e-11 5.437562553001207e-11 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1450 loss:  1.0\n",
      "outputs: 2.6301856526078637e-11 2.0103158721029857e-11 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1500 loss:  1.0\n",
      "outputs: 9.634183208151459e-12 7.429683604454063e-12 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1550 loss:  1.0\n",
      "outputs: 3.540434438675799e-12 2.7427322565887202e-12 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1600 loss:  1.0\n",
      "outputs: 1.2931407247090965e-12 1.0138447156457508e-12 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1650 loss:  1.0\n",
      "outputs: 4.735309908944496e-13 3.744623697492927e-13 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1700 loss:  1.0\n",
      "outputs: 1.7345036539863357e-13 1.3826979383506355e-13 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1750 loss:  1.0\n",
      "outputs: 6.345817697272355e-14 5.1066383109990565e-14 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1800 loss:  1.0\n",
      "outputs: 2.324582990291188e-14 1.8851492429258244e-14 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1850 loss:  1.0\n",
      "outputs: 8.512963181877154e-15 6.959006701239782e-15 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1900 loss:  1.0\n",
      "outputs: 3.1236667153024906e-15 2.5672340433504325e-15 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 1950 loss:  1.0\n",
      "outputs: 1.145954791867724e-15 9.470012579969106e-16 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2000 loss:  1.0\n",
      "outputs: 4.2000329859929455e-16 3.493937850832288e-16 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2050 loss:  1.0\n",
      "outputs: 1.5438627331120777e-16 1.2879000967125668e-16 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2100 loss:  1.0\n",
      "outputs: 5.670180162241156e-17 4.747816538346141e-17 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2150 loss:  1.0\n",
      "outputs: 2.0820313034610793e-17 1.750219050391763e-17 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2200 loss:  1.0\n",
      "outputs: 7.641942003476261e-18 6.452218881789204e-18 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2250 loss:  1.0\n",
      "outputs: 2.8065770826878137e-18 2.3781159301539746e-18 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2300 loss:  1.0\n",
      "outputs: 1.0310218003262812e-18 8.763897939889604e-19 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2350 loss:  1.0\n",
      "outputs: 3.7859787802852997e-19 3.2297450547831376e-19 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2400 loss:  1.0\n",
      "outputs: 1.3905964629706522e-19 1.1900756040255807e-19 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2450 loss:  1.0\n",
      "outputs: 5.1089902574368507e-20 4.3846120391528254e-20 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2500 loss:  1.0\n",
      "outputs: 1.875360135649356e-20 1.6157674334167746e-20 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2550 loss:  1.0\n",
      "outputs: 6.885996679549091e-21 5.95351654047356e-21 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2600 loss:  1.0\n",
      "outputs: 2.5295278047615702e-21 2.1933033215141646e-21 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2650 loss:  1.0\n",
      "outputs: 9.29415370282537e-22 8.079430087562012e-22 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2700 loss:  1.0\n",
      "outputs: 3.412417648223561e-22 2.9767382268589e-22 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2750 loss:  1.0\n",
      "outputs: 1.2534060723940302e-22 1.0965815341780518e-22 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2800 loss:  1.0\n",
      "outputs: 4.604712685774037e-23 4.0390255793498426e-23 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2850 loss:  1.0\n",
      "outputs: 1.6919706420790102e-23 1.4875644753061246e-23 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2900 loss:  1.0\n",
      "outputs: 6.21245672788869e-24 5.479776524185256e-24 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 2950 loss:  1.0\n",
      "outputs: 2.2821767681718765e-24 2.0182670994194868e-24 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3000 loss:  1.0\n",
      "outputs: 8.390344309453312e-25 7.431561779171563e-25 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3050 loss:  1.0\n",
      "outputs: 3.0839752880613524e-25 2.736537772612928e-25 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3100 loss:  1.0\n",
      "outputs: 1.1331815572771982e-25 1.0077533512743977e-25 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3150 loss:  1.0\n",
      "outputs: 4.1617189056670453e-26 3.711633391746824e-26 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3200 loss:  1.0\n",
      "outputs: 1.5301935572000399e-26 1.366527876451617e-26 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3250 loss:  1.0\n",
      "outputs: 5.622743228174802e-27 5.031991180753239e-27 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3300 loss:  1.0\n",
      "outputs: 2.066015347787158e-27 1.8529393180178984e-27 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3350 loss:  1.0\n",
      "outputs: 7.591578654677108e-28 6.823607348597892e-28 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3400 loss:  1.0\n",
      "outputs: 2.791305157403836e-28 2.5124107606746342e-28 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3450 loss:  1.0\n",
      "outputs: 1.0265152436804281e-28 9.249909299976487e-29 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3500 loss:  1.0\n",
      "outputs: 3.775416720016204e-29 3.405344466377336e-29 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3550 loss:  1.0\n",
      "outputs: 1.3878177452248738e-29 1.253865405547542e-29 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3600 loss:  1.0\n",
      "outputs: 5.1004741259580204e-30 4.6169731750015275e-30 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3650 loss:  1.0\n",
      "outputs: 1.8758585870196097e-30 1.6997469637748952e-30 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3700 loss:  1.0\n",
      "outputs: 6.897740195116681e-31 6.257696686630674e-31 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3750 loss:  1.0\n",
      "outputs: 2.5357561950910045e-31 2.303852553670952e-31 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3800 loss:  1.0\n",
      "outputs: 9.321837709965732e-32 8.482062782496764e-32 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3850 loss:  1.0\n",
      "outputs: 3.427742996365729e-32 3.122662833266593e-32 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3900 loss:  1.0\n",
      "outputs: 1.2605441589033632e-32 1.1495876308245151e-32 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 3950 loss:  1.0\n",
      "outputs: 4.63583233616838e-33 4.231807580951067e-33 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4000 loss:  1.0\n",
      "outputs: 1.7038017360291407e-33 1.5580070084065756e-33 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4050 loss:  1.0\n",
      "outputs: 6.26535458416906e-34 5.735305814284059e-34 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4100 loss:  1.0\n",
      "outputs: 2.3039985445591945e-34 2.111269587668409e-34 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4150 loss:  1.0\n",
      "outputs: 8.469731245234138e-35 7.772974187939757e-35 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4200 loss:  1.0\n",
      "outputs: 3.1150319050583635e-35 2.8613285983019245e-35 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4250 loss:  1.0\n",
      "outputs: 1.146008555879286e-35 1.0531139381632602e-35 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4300 loss:  1.0\n",
      "outputs: 4.213293464569557e-36 3.876643907204332e-36 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4350 loss:  1.0\n",
      "outputs: 1.549877535136241e-36 1.4268341132428416e-36 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4400 loss:  1.0\n",
      "outputs: 5.69841944153387e-37 5.252354078630417e-37 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4450 loss:  1.0\n",
      "outputs: 2.094988287794003e-37 1.9335309029422517e-37 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4500 loss:  1.0\n",
      "outputs: 7.705267737479999e-38 7.117079433312286e-38 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4550 loss:  1.0\n",
      "outputs: 2.8344579715732946e-38 2.6195660494721553e-38 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4600 loss:  1.0\n",
      "outputs: 1.0425718027813676e-38 9.642065683713916e-39 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4650 loss:  1.0\n",
      "outputs: 3.834178207445456e-39 3.548904668675138e-39 ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 4950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 5950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 6950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 7950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 8950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 9950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 10950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 11950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 12950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 13950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 14950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 15950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 16950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 17950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 18950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 19950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 20950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 21950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 22950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 23950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 24950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 25950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 26950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 27950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 28950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 29950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 30600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 30950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31800 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31850 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31900 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 31950 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32000 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32050 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32100 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32150 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32200 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32250 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32300 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32350 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32400 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32450 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32500 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32550 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32600 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32650 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32700 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n",
      "step 32750 loss:  nan\n",
      "outputs: nan nan ground_truth 0.20000000298023224 0.800000011920929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weight_network = Net()\n",
    "optimizer = optim.SGD(weight_network.parameters(), lr=0.01)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "for i, data in enumerate(dataloader):\n",
    "#     labels = data.mean(1).view(-1, 1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = weight_network(data)\n",
    "    \n",
    "    # Ground truth:\n",
    "    ground_truth = data.mean(1).view(-1, 1) \n",
    "    \n",
    "#     loss = (data.mean(1).view(-1, 1)-0.5) * (outputs/outputs.detach()).view(-1, 1)).sum()\n",
    "    loss = outputs/outputs.detach()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    if i % 50 == 0:\n",
    "        \n",
    "        w = weight_network(example_img)\n",
    "        print('step', i, 'loss: ', loss.item())\n",
    "        print('outputs:', w[0].item(), w[1].item(), 'ground_truth', example_img[0].item(), example_img[3].item())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "light grey = 0.2, dark grey = 0.8\n"
     ]
    }
   ],
   "source": [
    "visualize_img_batch(data)\n",
    "print('light grey = 0.2, dark grey = 0.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
