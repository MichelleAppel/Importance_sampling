{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance sampling in domain transfer: toy example batch sampling\n",
    "This is an toy example that demonstrates our importance sampling method used in unsupervised domain transfer. The problem we are addressing is that of the domains having non-matching distributions, which is often the case in GAN data sets. We use importance sampling to correct for this mis-match in domain distributions.\n",
    "\n",
    "For this example we use a color dataset, consisting of images of spatial size $1\\times1$. The idea is to have the domains consist of two modes: a dark and a light image. The goal is to correct for the dissimilar modes in the distribution using the weighting network in combination with importance sampling. The big assumption here is that dark and light images in the domains have semantic correspondence and that these modes are supposed to be equal.\n",
    "\n",
    "In unsupervised domain transfer, there exist no image pairs or ground truth for these kind of distributions: they are unknown. This is also the case in our example. To enforce semantic correspondence we add a simple static conditional generator that maps images to another color domain with the same brightness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for generation and visualization of the image batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_img_batch(batch):\n",
    "    '''Visualizes image batch\n",
    "    \n",
    "    Parameters:\n",
    "    batch (Tensor): An image batch\n",
    "    '''\n",
    "    grid = make_grid(batch, nrow=8, padding=1, normalize=False, range=None, scale_each=False, pad_value=0)\n",
    "    plt.imshow(grid.permute(1,2,0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_image_data(size=(2**21, 3, 1, 1), ratio=0.5, offset=0.2, color=0):\n",
    "    '''Generates a random image batch \n",
    "    consisting of two modes (dark and light images)\n",
    "    \n",
    "    Parameters:\n",
    "    size (tuple): The dimensions of the image batch (batch_size, channels, width, length)\n",
    "    ratio (float): The ratio of light to dark images\n",
    "    offset (float): The brightness of the images relative to black and bright\n",
    "    color (int): Red = 0, green = 1, blue = 2\n",
    "    \n",
    "    Returns:\n",
    "    image_batch (Tensor): The generated image batch\n",
    "    \n",
    "    '''\n",
    "    idx = torch.randperm(size[0])[:int(ratio*size[0])] # Randomly choose indices according to the ratio\n",
    "    image_batch = torch.zeros(size)\n",
    "    image_batch[:, color] += offset # light color\n",
    "    image_batch[idx, color] = 1 - offset # dark color \n",
    "    return image_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorDataset(Dataset):\n",
    "    '''The dataloader for the color images\n",
    "    '''\n",
    "    def __init__(self, ratio=0.5, offset=0.2, color=0):\n",
    "        '''  \n",
    "        Parameters:\n",
    "        ratio (float): The ratio of light to dark images\n",
    "        offset (float): The brightness of the images relative to black and bright\n",
    "        color (int): Red = 0, green = 1, blue = 2\n",
    "        '''\n",
    "        self.offset = offset\n",
    "        self.ratio = ratio\n",
    "        self.color = color\n",
    "        \n",
    "        self.dataset = random_image_data(ratio=self.ratio, \n",
    "                                         offset=self.offset, \n",
    "                                         color=self.color)\n",
    "        self.example_imgs = self.example()\n",
    "        \n",
    "    def example(self):\n",
    "        '''\n",
    "        Returns an example from each mode in the domain\n",
    "        \n",
    "        '''\n",
    "        example_imgs = torch.zeros(size=(2, 3, 1, 1))\n",
    "\n",
    "        example_imgs[0, self.color] += self.offset # light color\n",
    "        example_imgs[1, self.color] = 1 - self.offset # dark color\n",
    "        return example_imgs   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):      \n",
    "        return self.dataset[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightNet(nn.Module):\n",
    "    '''A simple network that predicts the importances of the samples'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(WeightNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 1)\n",
    "        self.fc2 = nn.Linear(1, 1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = torch.sigmoid(self.fc1(x.view(-1, 3)))\n",
    "        out = self.softmax(self.fc2(h1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialWeightNet(nn.Module):\n",
    "    '''A trivial network that predicts the importances of the samples'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TrivialWeightNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.ones((x.size()[0], 1))\n",
    "        out /= out.sum()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''A simple joint discriminator network'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(6, 3)\n",
    "        self.fc2 = nn.Linear(3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = torch.sigmoid(self.fc1(x.view(-1, 6)))\n",
    "        out = torch.sigmoid(self.fc2(h1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialDiscriminator(nn.Module):\n",
    "    '''A trivial joint discriminator network'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TrivialDiscriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.zeros(x.size()[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''A simple conditional generator network'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 3)\n",
    "        self.fc2 = nn.Linear(3, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x.view(-1, 3)))\n",
    "        out = torch.sigmoid(self.fc2(h1))\n",
    "        return out.unsqueeze(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialGenerator(nn.Module):\n",
    "    '''A trivial conditional generator network'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TrivialGenerator, self).__init__()\n",
    "        self.nn = nn.Linear(1,1) # Otherwise error\n",
    "        \n",
    "    def forward(self, x):\n",
    "        new = torch.zeros((x.shape))\n",
    "        new[:, 0] = x[:, 1]\n",
    "        new[:, 1] = x[:, 0]\n",
    "        return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A demonstration of the two domains\n",
    "The idea here is that we create two domains: A red and a green domain. The domains consist of two modes: dark and light images. The big assumption here for our purpose is that the dark and light images have semantic correspondance, so for example: dark red will be translated to dark green.\n",
    "\n",
    "We can adjust the intensity of the images (offset) and the ratio of the modes for the experiment. In the visualiztion you can see the elements of the dataset. In the histogram you can see the modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Settings for domain A (red)\n",
    "offset_A = 0.2\n",
    "ratio_A = 0.5\n",
    "color_A = 0\n",
    "dataset_A = ColorDataset(offset=offset_A, ratio=ratio_A, color=color_A)\n",
    "dataloader_A = DataLoader(dataset_A, batch_size=256, shuffle=True)\n",
    "\n",
    "# Settings for domain B (green)\n",
    "offset_B = 0.2\n",
    "ratio_B = 0.5\n",
    "color_B = 1\n",
    "dataset_B = ColorDataset(offset=offset_B, ratio=ratio_B, color=color_B)\n",
    "dataloader_B = DataLoader(dataset_B, batch_size=256, shuffle=True)\n",
    "\n",
    "visualize_img_batch(dataset_A.example_imgs)\n",
    "visualize_img_batch(dataset_B.example_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ratio domain A: 0.5\n",
      "Count: tensor([132, 124]) Ratio in this batch: tensor([0.5156, 0.4844])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ratio domain B: 0.5\n",
      "Count: tensor([133, 123]) Ratio in this batch: tensor([0.5195, 0.4805])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_hist(data):\n",
    "    data = data.view(3, -1).sum(0)\n",
    "    plt.hist(data, weights=torch.ones(len(data))/len(data))\n",
    "    x_unique_count = torch.stack([(data==x_u).sum() for x_u in data.unique()])\n",
    "    \n",
    "    print('Count:', x_unique_count, 'Ratio in this batch:', x_unique_count/float(x_unique_count.sum()))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for i, (data_A, data_B) in enumerate(zip(dataloader_A, dataloader_B)):\n",
    "    print('True ratio domain A:', dataloader_A.dataset.ratio)\n",
    "    plot_hist(data_A)\n",
    "    print('True ratio domain B:', dataloader_B.dataset.ratio)\n",
    "    plot_hist(data_B)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss_g:  -8.80419921875 loss_d:  8.80419921875 loss_w:  77.51392364501953\n",
      "step 500 loss_g:  0.1690673828125 loss_d:  -0.1690673828125 loss_w:  0.02858377993106842\n",
      "step 1000 loss_g:  -0.034912109375 loss_d:  0.034912109375 loss_w:  0.0012188553810119629\n",
      "step 1500 loss_g:  0.0185546875 loss_d:  -0.0185546875 loss_w:  0.00034427642822265625\n",
      "step 2000 loss_g:  -0.03125 loss_d:  0.03125 loss_w:  0.0009765625\n",
      "step 2500 loss_g:  0.01171875 loss_d:  -0.01171875 loss_w:  0.0001373291015625\n",
      "step 3000 loss_g:  -0.0015869140625 loss_d:  0.0015869140625 loss_w:  2.518296241760254e-06\n",
      "step 3500 loss_g:  -0.001220703125 loss_d:  0.001220703125 loss_w:  1.4901161193847656e-06\n",
      "step 4000 loss_g:  0.001708984375 loss_d:  -0.001708984375 loss_w:  2.9206275939941406e-06\n",
      "step 4500 loss_g:  -0.0006103515625 loss_d:  0.0006103515625 loss_w:  3.725290298461914e-07\n",
      "step 5000 loss_g:  -0.0050048828125 loss_d:  0.0050048828125 loss_w:  2.504885196685791e-05\n",
      "step 5500 loss_g:  0.003662109375 loss_d:  -0.003662109375 loss_w:  1.341104507446289e-05\n",
      "step 6000 loss_g:  -0.003173828125 loss_d:  0.003173828125 loss_w:  1.0073184967041016e-05\n",
      "step 6500 loss_g:  0.054443359375 loss_d:  -0.054443359375 loss_w:  0.0029640793800354004\n",
      "step 7000 loss_g:  -0.0028076171875 loss_d:  0.0028076171875 loss_w:  7.88271427154541e-06\n",
      "step 7500 loss_g:  0.001953125 loss_d:  -0.001953125 loss_w:  3.814697265625e-06\n",
      "step 8000 loss_g:  0.001953125 loss_d:  -0.001953125 loss_w:  3.814697265625e-06\n",
      "step 0 loss_g:  -0.0009765625 loss_d:  0.0009765625 loss_w:  9.5367431640625e-07\n",
      "step 500 loss_g:  0.0093994140625 loss_d:  -0.0093994140625 loss_w:  8.834898471832275e-05\n",
      "step 1000 loss_g:  0.002685546875 loss_d:  -0.002685546875 loss_w:  7.212162017822266e-06\n",
      "step 1500 loss_g:  0.0001220703125 loss_d:  -0.0001220703125 loss_w:  1.4901161193847656e-08\n",
      "step 2000 loss_g:  -0.004150390625 loss_d:  0.004150390625 loss_w:  1.722574234008789e-05\n",
      "step 2500 loss_g:  0.0 loss_d:  0.0 loss_w:  0.0\n",
      "step 3000 loss_g:  -0.0009765625 loss_d:  0.0009765625 loss_w:  9.5367431640625e-07\n",
      "step 3500 loss_g:  -0.0028076171875 loss_d:  0.0028076171875 loss_w:  7.88271427154541e-06\n",
      "step 4000 loss_g:  0.0001220703125 loss_d:  -0.0001220703125 loss_w:  1.4901161193847656e-08\n",
      "step 4500 loss_g:  -0.000244140625 loss_d:  0.000244140625 loss_w:  5.960464477539063e-08\n",
      "step 5000 loss_g:  -0.0001220703125 loss_d:  0.0001220703125 loss_w:  1.4901161193847656e-08\n",
      "step 5500 loss_g:  0.0006103515625 loss_d:  -0.0006103515625 loss_w:  3.725290298461914e-07\n",
      "step 6000 loss_g:  -0.0008544921875 loss_d:  0.0008544921875 loss_w:  7.301568984985352e-07\n",
      "step 6500 loss_g:  0.0003662109375 loss_d:  -0.0003662109375 loss_w:  1.341104507446289e-07\n",
      "step 7000 loss_g:  0.000732421875 loss_d:  -0.000732421875 loss_w:  5.364418029785156e-07\n",
      "step 7500 loss_g:  -0.001708984375 loss_d:  0.001708984375 loss_w:  2.9206275939941406e-06\n",
      "step 8000 loss_g:  -0.00048828125 loss_d:  0.00048828125 loss_w:  2.384185791015625e-07\n",
      "step 0 loss_g:  0.00048828125 loss_d:  -0.00048828125 loss_w:  2.384185791015625e-07\n",
      "step 500 loss_g:  -0.0006103515625 loss_d:  0.0006103515625 loss_w:  3.725290298461914e-07\n",
      "step 1000 loss_g:  0.000732421875 loss_d:  -0.000732421875 loss_w:  5.364418029785156e-07\n",
      "step 1500 loss_g:  0.00244140625 loss_d:  -0.00244140625 loss_w:  5.9604644775390625e-06\n",
      "step 2000 loss_g:  0.001708984375 loss_d:  -0.001708984375 loss_w:  2.9206275939941406e-06\n",
      "step 2500 loss_g:  -0.0001220703125 loss_d:  0.0001220703125 loss_w:  1.4901161193847656e-08\n",
      "step 3000 loss_g:  0.001953125 loss_d:  -0.001953125 loss_w:  3.814697265625e-06\n",
      "step 3500 loss_g:  0.0013427734375 loss_d:  -0.0013427734375 loss_w:  1.8030405044555664e-06\n",
      "step 4000 loss_g:  0.00079345703125 loss_d:  -0.00079345703125 loss_w:  6.295740604400635e-07\n",
      "step 4500 loss_g:  0.0 loss_d:  0.0 loss_w:  0.0\n",
      "step 5000 loss_g:  -0.00146484375 loss_d:  0.00146484375 loss_w:  2.1457672119140625e-06\n",
      "step 5500 loss_g:  -0.00054931640625 loss_d:  0.00054931640625 loss_w:  3.0174851417541504e-07\n",
      "step 6000 loss_g:  -0.00048828125 loss_d:  0.00048828125 loss_w:  2.384185791015625e-07\n",
      "step 6500 loss_g:  0.00048828125 loss_d:  -0.00048828125 loss_w:  2.384185791015625e-07\n",
      "step 7000 loss_g:  0.0006103515625 loss_d:  -0.0006103515625 loss_w:  3.725290298461914e-07\n",
      "step 7500 loss_g:  0.00091552734375 loss_d:  -0.00091552734375 loss_w:  8.381903171539307e-07\n",
      "step 8000 loss_g:  0.001220703125 loss_d:  -0.001220703125 loss_w:  1.4901161193847656e-06\n",
      "step 0 loss_g:  0.0013427734375 loss_d:  -0.0013427734375 loss_w:  1.8030405044555664e-06\n",
      "step 500 loss_g:  0.0008544921875 loss_d:  -0.0008544921875 loss_w:  7.301568984985352e-07\n",
      "step 1000 loss_g:  0.00018310546875 loss_d:  -0.00018310546875 loss_w:  3.3527612686157227e-08\n",
      "step 1500 loss_g:  -0.0006103515625 loss_d:  0.0006103515625 loss_w:  3.725290298461914e-07\n",
      "step 2000 loss_g:  0.00018310546875 loss_d:  -0.00018310546875 loss_w:  3.3527612686157227e-08\n",
      "step 2500 loss_g:  -6.103515625e-05 loss_d:  6.103515625e-05 loss_w:  3.725290298461914e-09\n",
      "step 3000 loss_g:  3.0517578125e-05 loss_d:  -3.0517578125e-05 loss_w:  9.313225746154785e-10\n",
      "step 3500 loss_g:  -3.0517578125e-05 loss_d:  3.0517578125e-05 loss_w:  9.313225746154785e-10\n",
      "step 4000 loss_g:  0.0001220703125 loss_d:  -0.0001220703125 loss_w:  1.4901161193847656e-08\n",
      "step 4500 loss_g:  -0.00018310546875 loss_d:  0.00018310546875 loss_w:  3.3527612686157227e-08\n",
      "step 5000 loss_g:  0.0001983642578125 loss_d:  -0.0001983642578125 loss_w:  3.934837877750397e-08\n",
      "step 5500 loss_g:  -0.00012969970703125 loss_d:  0.00012969970703125 loss_w:  1.682201400399208e-08\n",
      "step 6000 loss_g:  6.866455078125e-05 loss_d:  -6.866455078125e-05 loss_w:  4.71482053399086e-09\n",
      "step 6500 loss_g:  0.0 loss_d:  0.0 loss_w:  0.0\n",
      "step 7000 loss_g:  0.0001678466796875 loss_d:  -0.0001678466796875 loss_w:  2.8172507882118225e-08\n",
      "step 7500 loss_g:  -6.103515625e-05 loss_d:  6.103515625e-05 loss_w:  3.725290298461914e-09\n",
      "step 8000 loss_g:  -0.0001373291015625 loss_d:  0.0001373291015625 loss_w:  1.885928213596344e-08\n",
      "step 0 loss_g:  -1.52587890625e-05 loss_d:  1.52587890625e-05 loss_w:  2.3283064365386963e-10\n"
     ]
    }
   ],
   "source": [
    "# Initialize the networks\n",
    "weight_network_A = TrivialWeightNet()\n",
    "weight_network_B = TrivialWeightNet()\n",
    "generator_A = Generator()\n",
    "generator_B = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Initialize the optimizers\n",
    "optimizer_w = optim.Adam(itertools.chain(weight_network_A.parameters(), \n",
    "                                         weight_network_B.parameters()), lr=0.01)\n",
    "optimizer_g = optim.Adam(itertools.chain(generator_A.parameters(),\n",
    "                                         generator_B.parameters()), lr=0.01)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.01)\n",
    "\n",
    "# Store values\n",
    "samples_A = []\n",
    "samples_B = []\n",
    "\n",
    "losses_g = []\n",
    "losses_d = []\n",
    "losses_w = []\n",
    "\n",
    "Lminusses = []\n",
    "Lplusses = []\n",
    "\n",
    "example_importances_A = []\n",
    "example_importances_B = []\n",
    "\n",
    "\n",
    "sampled_batch_size = 64 # The amount of images sampled using importance sampling\n",
    "for epoch in range(5):\n",
    "    for i, (data_A, data_B) in enumerate(zip(dataloader_A, dataloader_B)):\n",
    "        # Set gradients to zero\n",
    "        optimizer_w.zero_grad()\n",
    "        optimizer_d.zero_grad()\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        # The sampling process ----------------------------------------------------------------------------\n",
    "        importances_A = weight_network_A(data_A).squeeze() # Get the importances for each image in domain A\n",
    "        importances_B = weight_network_B(data_B).squeeze() # Get the importances for each image in domain B\n",
    "\n",
    "        sampled_idx_A = list( # Sample from batch A according to these importances\n",
    "            torch.utils.data.sampler.WeightedRandomSampler(importances_A, \n",
    "                                                           sampled_batch_size, \n",
    "                                                           replacement=False))\n",
    "\n",
    "        sampled_importances_A = importances_A[sampled_idx_A] # The importances assigned to the smaller batch\n",
    "        real_A = data_A[sampled_idx_A] # The sampled smaller batch A\n",
    "\n",
    "        sampled_idx_B = list( # Sample from batch Baccording to these importances\n",
    "            torch.utils.data.sampler.WeightedRandomSampler(importances_B,\n",
    "                                                           sampled_batch_size, \n",
    "                                                           replacement=False))\n",
    "\n",
    "        sampled_importances_B = importances_B[sampled_idx_B] # The importances assigned to the smaller batch\n",
    "        real_B = data_B[sampled_idx_B] # The sampled smaller batch B\n",
    "        # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Generated images\n",
    "        fake_A = generator_A(real_B)\n",
    "        fake_B = generator_B(real_A)\n",
    "\n",
    "        # Discriminator values\n",
    "        discriminated_A = discriminator(torch.cat((real_A, fake_B)))\n",
    "        discriminated_B = discriminator(torch.cat((fake_A, real_B)))\n",
    "\n",
    "        # The loss function --------------------------------------------------------------------------------\n",
    "        Lmin = (discriminated_A * sampled_importances_A/sampled_importances_A.detach()).sum()\n",
    "        Lplus = (discriminated_B * sampled_importances_B/sampled_importances_B.detach()).sum()\n",
    "\n",
    "        loss_g = Lmin - Lplus\n",
    "        loss_w = (Lmin - Lplus)**2\n",
    "        loss_d = Lplus - Lmin\n",
    "        # ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Backward\n",
    "        loss_g.backward(retain_graph=True)\n",
    "        loss_w.backward(retain_graph=True)\n",
    "        loss_d.backward()\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer_g.step()\n",
    "        optimizer_w.step()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Store values --------------------------------------------------------------------------------------\n",
    "        Lminusses += [Lmin.item()]\n",
    "        Lplusses += [Lplus.item()]\n",
    "\n",
    "        losses_g += [loss_g.item()]\n",
    "        losses_d += [loss_d.item()]\n",
    "        losses_w += [loss_w.item()]\n",
    "\n",
    "        w_a = weight_network_A(dataset_A.example_imgs)\n",
    "        w_b = weight_network_B(dataset_B.example_imgs)\n",
    "        example_importances_A += [(w_a[0].item(), w_a[1].item())] # Store examples in a list\n",
    "        example_importances_B += [(w_b[0].item(), w_b[1].item())] # Store examples in a list\n",
    "        # ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Print statistics\n",
    "        if i % 500 == 0:\n",
    "            samples_A += [fake_A.detach()]\n",
    "            samples_B += [fake_B.detach()]\n",
    "            print('step', i, 'loss_g: ', loss_g.item(), 'loss_d: ', loss_d.item(), 'loss_w: ', loss_w.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_img_batch(real_A)\n",
    "visualize_img_batch(fake_B.detach())\n",
    "print('True ratio domain A {}'.format(ratio_A))\n",
    "plot_hist(real_A)\n",
    "\n",
    "visualize_img_batch(real_B)\n",
    "visualize_img_batch(fake_A.detach())\n",
    "print('True ratio domain B {}'.format(ratio_B))\n",
    "plot_hist(real_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Losses over iterations')\n",
    "plt.xlabel('Training iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(losses_g)\n",
    "plt.plot(losses_d)\n",
    "plt.plot(losses_w)\n",
    "plt.legend(['G', 'D', 'W'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Losses over iterations')\n",
    "plt.xlabel('Training iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(Lminusses)\n",
    "plt.plot(Lplusses)\n",
    "plt.legend(['Lmin', 'Lplus'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Assigned importances for the toy example images over the course of training')\n",
    "plt.plot(example_importances_A)\n",
    "plt.plot(example_importances_B)\n",
    "plt.legend(['Img A with value {} (p={})'.format(offset_A, ratio_A), \n",
    "            'Img A with value {} (p={})'.format(1-offset_A, 1-ratio_A), \n",
    "            'Img B with value {} (p={})'.format(offset_B, ratio_B), \n",
    "            'Img B with value {} (p={})'.format(1-offset_B, 1-ratio_B)])\n",
    "plt.xlabel('Assigned importance')\n",
    "plt.ylabel('Training iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Ratio between importances of the same mode')\n",
    "plt.plot(torch.Tensor(example_importances_A)[:, 0]/torch.Tensor(example_importances_B)[:, 0])\n",
    "plt.plot(torch.Tensor(example_importances_A)[:, 1]/torch.Tensor(example_importances_B)[:, 1])\n",
    "plt.legend(['Dark images', 'Light images'])\n",
    "plt.xlabel('Ratio importance')\n",
    "plt.ylabel('Training iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in samples_B:\n",
    "    visualize_img_batch(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
